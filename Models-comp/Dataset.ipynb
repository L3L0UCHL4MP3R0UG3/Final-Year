{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dataset.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Dataset : https://www.kaggle.com/eswarchandt/website-phishing , phishing.csv."
      ],
      "metadata": {
        "id": "A1cJEk1Yad7X"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrWoyKY6gbN_"
      },
      "source": [
        "#Important!!\n",
        "import pandas as pd\n",
        "df=pd.read_csv(\"Phishing_Legitimate_full.csv\")\n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns\n"
      ],
      "metadata": {
        "id": "e1shMdpU6z80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNL3CMaDjFey"
      },
      "source": [
        "df1=pd.read_csv(\"phishing.csv\")\n",
        "print(df1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYoJWb0DjI75"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHfvIoRCjR3D"
      },
      "source": [
        "df1.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsjK34CrjUZ5"
      },
      "source": [
        "import pandas as pd\n",
        "pd.read_csv(\"phishtank.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdgV2_WnjeMC"
      },
      "source": [
        "df2=pd.read_csv(\"phishtank.csv\")\n",
        "print(df2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6q4s52EjjJ9"
      },
      "source": [
        "df2.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCqepXNAjlka"
      },
      "source": [
        "df2.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Spliting Dataset into training and validation sets."
      ],
      "metadata": {
        "id": "D2ZDrXtNbRkD"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFgXyjOkknJ-"
      },
      "source": [
        "#Important!!\n",
        "from sklearn.model_selection import train_test_split,cross_val_score\n",
        "\n",
        "X= df.drop(columns='id')\n",
        "X= X.drop(columns='CLASS_LABEL')\n",
        "X.head()\n",
        "\n",
        "Y=df['CLASS_LABEL']\n",
        "Y=pd.DataFrame(Y)\n",
        "Y.head()\n",
        "\n",
        "train_X,test_X,train_Y,test_Y=train_test_split(X,Y,test_size=0.3,random_state=2)\n",
        "print(train_X.shape)\n",
        "print(test_X.shape)\n",
        "print(train_Y.shape)\n",
        "print(test_Y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.head()"
      ],
      "metadata": {
        "id": "5YOjj5Bt71zo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Important!!\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "Yj7bR_u0U49a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Imp.\n",
        "\n",
        "def plot_confusion_matrix(test_Y, predict_y,color):\n",
        " C = confusion_matrix(test_Y, predict_y)\n",
        " A =(((C.T)/(C.sum(axis=1))).T)\n",
        " B =(C/C.sum(axis=0))\n",
        " plt.figure(figsize=(20,4))\n",
        " labels = [1,2]\n",
        " cmap=sns.light_palette(color)\n",
        " plt.subplot(1, 3, 1)\n",
        " sns.heatmap(C, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n",
        " plt.xlabel('Predicted Class')\n",
        " plt.ylabel('Original Class')\n",
        " plt.title(\"Confusion matrix\")\n",
        " plt.subplot(1, 3, 2)\n",
        " sns.heatmap(B, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n",
        " plt.xlabel('Predicted Class')\n",
        " plt.ylabel('Original Class')\n",
        " plt.title(\"Precision matrix\")\n",
        " plt.subplot(1, 3, 3)\n",
        " sns.heatmap(A, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n",
        " plt.xlabel('Predicted Class')\n",
        " plt.ylabel('Original Class')\n",
        " plt.title(\"Recall matrix\")\n",
        " plt.show()"
      ],
      "metadata": {
        "id": "vfWUwBtZUn1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Decision Tree"
      ],
      "metadata": {
        "id": "Hg8Z0S01bYTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#imp.\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
        "\n",
        "dtree=DecisionTreeClassifier()\n",
        "model_3=dtree.fit(train_X,train_Y)\n",
        "dtree_predict=model_3.predict(test_X)\n",
        "accuracy_score(dtree_predict,test_Y)"
      ],
      "metadata": {
        "id": "vl_lyW9ZTrQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(dtree_predict,test_Y))"
      ],
      "metadata": {
        "id": "1m24IThNUcjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(test_Y, dtree_predict,\"purple\")"
      ],
      "metadata": {
        "id": "GQzSQxjAUhn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Random Forest"
      ],
      "metadata": {
        "id": "vO5c-yogbcOR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rfc=RandomForestClassifier()\n",
        "model_4=rfc.fit(train_X,train_Y)\n",
        "\n",
        "rfc_predict=model_4.predict(test_X)\n",
        "accuracy_score(rfc_predict,test_Y)"
      ],
      "metadata": {
        "id": "DM7rn7qlZpra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(rfc_predict,test_Y))"
      ],
      "metadata": {
        "id": "hJXNhCuvZ0tn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(test_Y, rfc_predict,\"green\")"
      ],
      "metadata": {
        "id": "bTL9h-NZZ1dm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sequential Model"
      ],
      "metadata": {
        "id": "Jw7SHsvANJxy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(X)"
      ],
      "metadata": {
        "id": "U2qfxpT9OUkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "FaRaPxIEWQg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model_1 = tf.keras.models.Sequential([\n",
        "  \n",
        "  tf.keras.layers.Dense(310, activation='relu', input_shape=(31,)),\n",
        "  tf.keras.layers.Dense(310, activation='relu'),\n",
        "  tf.keras.layers.Dense(310, activation='relu'),\n",
        "  tf.keras.layers.Dense(31, activation='relu'),\n",
        "  \n",
        "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "hVTmkUoJNO4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1.summary()"
      ],
      "metadata": {
        "id": "QwCnvACDPmC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "model_1.compile(loss='binary_crossentropy',\n",
        "              optimizer=RMSprop(learning_rate=0.05),\n",
        "              #optimizer='sgd',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "v0_cm2QaPoTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if(logs.get('val_accuracy')>=0.80):\n",
        "      print(\"\\nReached 99% accuracy so cancelling training!\")\n",
        "      self.model.stop_training = True\n",
        "callback=myCallback()"
      ],
      "metadata": {
        "id": "eH3I-9UTPv2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " model_1.fit(\n",
        "      train_X, train_Y,\n",
        "      steps_per_epoch=8,  \n",
        "      epochs=5000,\n",
        "      verbose=1,\n",
        "      validation_data = (test_X,test_Y),\n",
        "      validation_steps=8,\n",
        "      callbacks=[callback])"
      ],
      "metadata": {
        "id": "ldL-BHHVP_by"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_X"
      ],
      "metadata": {
        "id": "nMkmYDpZUd8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TabNet"
      ],
      "metadata": {
        "id": "I7NoeG0I_Wv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_tabnet"
      ],
      "metadata": {
        "id": "jkB1EKZmCLKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "\n",
        "import torch\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "dY1ElXrYrvwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('Phishing_Legitimate_full.csv')"
      ],
      "metadata": {
        "id": "4Zm7onNlKeAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unused_feat = ['CLASS_LABEL']\n",
        "features = [ col for col in df.columns if col not in unused_feat] \n",
        "\n",
        "cat_idxs = [ i for i, f in enumerate(features) if f in df.columns]\n",
        "\n",
        "#cat_dims = [ categorical_dims[f] for i, f in enumerate(features) if f in df.columns]\n",
        "\n",
        "tabnet_params = {\"cat_idxs\":cat_idxs,\n",
        "                 #\"cat_dims\":cat_dims,\n",
        "                 \"cat_emb_dim\":1,\n",
        "                 \"optimizer_fn\":torch.optim.Adam,\n",
        "                 \"optimizer_params\":dict(lr=2e-2),\n",
        "                 \"scheduler_params\":{\"step_size\":50, # how to use learning rate scheduler\n",
        "                                 \"gamma\":0.9},\n",
        "                 \"scheduler_fn\":torch.optim.lr_scheduler.StepLR,\n",
        "                 \"mask_type\":'entmax' # \"sparsemax\"\n",
        "                }\n",
        "\n",
        "clf = TabNetClassifier(**tabnet_params\n",
        "                      )"
      ],
      "metadata": {
        "id": "fFDabTwLEoxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_epochs = 100\n",
        "save_history = []\n",
        "for _ in range(2):\n",
        "    clf.fit(\n",
        "        X_train=train_X, y_train=train_Y,\n",
        "        eval_set=[(train_X, train_Y), (test_X, test_Y)],\n",
        "        eval_name=['train', 'valid'],\n",
        "        eval_metric=['acc'],\n",
        "        max_epochs=max_epochs , patience=20,\n",
        "        batch_size=1024, virtual_batch_size=128,\n",
        "        num_workers=0,\n",
        "        weights=1,\n",
        "        drop_last=False\n",
        "    )\n",
        "    save_history.append(clf.history[\"valid_auc\"])\n",
        "    \n",
        "assert(np.all(np.array(save_history[0]==np.array(save_history[1]))))"
      ],
      "metadata": {
        "id": "yDKpxXU5C7kT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##XBNet"
      ],
      "metadata": {
        "id": "xHvj6TwqY2_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade git+https://github.com/tusharsarkar3/XBNet.git"
      ],
      "metadata": {
        "id": "NiYSc5CPY5pN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from XBNet.training_utils import training,predict\n",
        "from XBNet.models import XBNETClassifier\n",
        "from XBNet.run import run_XBNET \n",
        "import pandas as pd \n",
        "data = pd.read_csv('Phishing_Legitimate_full.csv')\n",
        "data.head()"
      ],
      "metadata": {
        "id": "x82D9Ov5aGqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=data.drop(['id'],axis=1)\n",
        "data.head()"
      ],
      "metadata": {
        "id": "e4G5txorbG3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y=data[['CLASS_LABEL']].to_numpy()\n",
        "x=data.loc[:,:'PctExtNullSelfRedirectHyperlinksRT'].to_numpy()\n",
        "\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size= 0.20, random_state= True,stratify=y) \n",
        "x_train.shape,x_test.shape,y_train.shape,y_test.shape"
      ],
      "metadata": {
        "id": "2dCvjuAvbTgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train=y_train.reshape((-1))\n",
        "\n",
        "\n",
        "y_test=y_test.reshape((-1))\n"
      ],
      "metadata": {
        "id": "JixSJAPObuxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = XBNETClassifier(x_train,y_train,num_layers=3)"
      ],
      "metadata": {
        "id": "Zsjup8QHcGmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.05) \n",
        "m,acc, lo, val_ac, val_lo = run_XBNET(x_train,x_test,y_train,y_test,model,criterion,optimizer,epochs=24,batch_size=10)"
      ],
      "metadata": {
        "id": "m6-nl6oJdAHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TabNet 2.0\n"
      ],
      "metadata": {
        "id": "UNohp8h71PaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_addons"
      ],
      "metadata": {
        "id": "TX8KMq6j1O0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import multiprocessing as mp\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from tensorflow_addons.activations import sparsemax\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "tkZjDhg21xZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def GLU(x):\n",
        "    return x * tf.sigmoid(x)\n",
        "\n",
        "class FCBlock(layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super().__init__()\n",
        "        self.layer = layers.Dense(units)\n",
        "        self.bn = layers.BatchNormalization()\n",
        "\n",
        "    def call(self, x):\n",
        "        return GLU(self.bn(self.layer(x)))"
      ],
      "metadata": {
        "id": "kCd6OSGT2VYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SharedBlock(layers.Layer):\n",
        "    def __init__(self, units, mult=tf.sqrt(0.5)):\n",
        "        super().__init__()\n",
        "        self.layer1 = FCBlock(units)\n",
        "        self.layer2 = FCBlock(units)\n",
        "        self.mult = mult\n",
        "\n",
        "    def call(self, x):\n",
        "        out1 = self.layer1(x)\n",
        "        out2 = self.layer2(out1)\n",
        "        return out2 + self.mult * out1\n",
        "\n",
        "class DecisionBlock(SharedBlock):\n",
        "    def __init__(self, units, mult=tf.sqrt(0.5)):\n",
        "        super().__init__(units, mult)\n",
        "\n",
        "    def call(self, x):\n",
        "        out1 = x * self.mult + self.layer1(x)\n",
        "        out2 = out1 * self.mult + self.layer2(out1)\n",
        "        return out2"
      ],
      "metadata": {
        "id": "izD-OCon2r75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Prior(layers.Layer):\n",
        "    def __init__(self, gamma=1.1):\n",
        "        super().__init__()\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def reset(self):\n",
        "        self.P = 1.0\n",
        "\n",
        "    def call(self, mask):\n",
        "        self.P = self.P * (self.gamma - mask)\n",
        "        return self.P"
      ],
      "metadata": {
        "id": "AVAdunB83Q4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentiveTransformer(layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super().__init__()\n",
        "        self.layer = layers.Dense(units)\n",
        "        self.bn = layers.BatchNormalization()\n",
        "\n",
        "    def call(self, x, prior):\n",
        "        return sparsemax(prior * self.bn(self.layer(x)))"
      ],
      "metadata": {
        "id": "yuprkDyF3aan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p = np.arange(0.01, 1, 0.01)\n",
        "e = - p * np.log(p)\n",
        "plt.plot(p, e)\n",
        "plt.xlabel(\"mask\")\n",
        "plt.ylabel(\"entropy\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "r1N04EmJ4PPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TabNet(keras.Model):\n",
        "    def __init__(self, input_dim, output_dim, steps, n_d, n_a, gamma=1.3):\n",
        "        super().__init__()\n",
        "        # hyper-parameters\n",
        "        self.n_d, self.n_a, self.steps = n_d, n_a, steps\n",
        "        # input-normalisation\n",
        "        self.bn = layers.BatchNormalization()\n",
        "        # Feature Transformer\n",
        "        self.shared = SharedBlock(n_d+n_a)\n",
        "        self.first_block = DecisionBlock(n_d+n_a)\n",
        "        self.decision_blocks = [DecisionBlock(n_d+n_a)] * steps\n",
        "        # Attentive Transformer\n",
        "        self.attention = [AttentiveTransformer(input_dim)] * steps\n",
        "        self.prior_scale = Prior(gamma)\n",
        "        # final layer\n",
        "        self.final = layers.Dense(output_dim)\n",
        "\n",
        "        self.eps = 1e-8\n",
        "        self.add_layer = layers.Add()\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, x):\n",
        "        self.prior_scale.reset()\n",
        "        final_outs = []\n",
        "        mask_losses = []\n",
        "\n",
        "        x = self.bn(x)\n",
        "        attention = self.first_block(self.shared(x))[:,:self.n_a]\n",
        "        for i in range(self.steps):\n",
        "            mask = self.attention[i](attention, self.prior_scale.P)\n",
        "            entropy = mask * tf.math.log(mask + self.eps)\n",
        "            mask_losses.append(\n",
        "                -tf.reduce_sum(entropy, axis=-1) / self.steps\n",
        "            )\n",
        "\n",
        "            prior = self.prior_scale(mask)\n",
        "            out = self.decision_blocks[i](self.shared(x * prior))\n",
        "            attention, output = out[:,:self.n_a], out[:,self.n_a:]\n",
        "            final_outs.append(tf.nn.relu(output))\n",
        "\n",
        "        final_out = self.add_layer(final_outs)\n",
        "        mask_loss = self.add_layer(mask_losses)\n",
        "\n",
        "        return self.final(final_out), mask_loss\n",
        "\n",
        "    def mask_importance(self, x):\n",
        "        self.prior_scale.reset()\n",
        "        feature_importance = 0\n",
        "\n",
        "        x = self.bn(x)\n",
        "        attention = self.first_block(self.shared(x))[:,:self.n_a]\n",
        "        for i in range(self.steps):\n",
        "            mask = self.attention[i](attention, self.prior_scale.P)\n",
        "\n",
        "            prior = self.prior_scale(mask)\n",
        "            out = self.decision_blocks[i](self.shared(x * prior))\n",
        "            attention, output = out[:,:self.n_a], out[:,self.n_a:]\n",
        "            step_importance = tf.reduce_sum(tf.nn.relu(output), axis=1, keepdims=True)\n",
        "            feature_importance += mask * step_importance\n",
        "\n",
        "        return feature_importance"
      ],
      "metadata": {
        "id": "4Fuc16dQ4RWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# collapse\n",
        "from keras.losses import SparseCategoricalCrossentropy\n",
        "\n",
        "sce = SparseCategoricalCrossentropy(from_logits=True)\n",
        "reg_sparse = 0.01\n",
        "def full_loss(y_true, y_pred):\n",
        "    logits, mask_loss = y_pred\n",
        "    return sce(y_true, logits) + reg_sparse * mask_loss.mean()"
      ],
      "metadata": {
        "id": "OJWWXRjR4X3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mask_loss(y_true, mask_losses):\n",
        "    return tf.reduce_mean(mask_losses)\n",
        "\n",
        "model = TabNet(30, 2, 2, 2, 2, 1.3)\n",
        "model.compile(\n",
        "    'Adam', \n",
        "    loss=[sce, mask_loss],\n",
        "    loss_weights=[1, 0.01]\n",
        ")\n",
        "model.fit(\n",
        "    train_X, \n",
        "    train_Y, \n",
        "    epochs=5, \n",
        "    batch_size=256\n",
        ")"
      ],
      "metadata": {
        "id": "D9ia791y5lWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##DBN   https://github.com/albertbup/deep-belief-network"
      ],
      "metadata": {
        "id": "K5VOUCiU-aFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/albertbup/deep-belief-network.git"
      ],
      "metadata": {
        "id": "XXnPT-1I-dHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd deep-belief-network"
      ],
      "metadata": {
        "id": "c6dr2ygfA_pK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r deep-belief-network/requirements.txt"
      ],
      "metadata": {
        "id": "3TuM_n9JBPDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "I4Uu_LUqCQ-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/deep-belief-network/dbn/activations.py\n",
        "!python /content/deep-belief-network/dbn/utils.py\n"
      ],
      "metadata": {
        "id": "K1bUiIAtB7vP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/deep-belief-network/dbn/tensorflow/__init__.py"
      ],
      "metadata": {
        "id": "v3MP-wLZQ422"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/deep-belief-network/dbn/tensorflow/models.py"
      ],
      "metadata": {
        "id": "aYEWSBy3EtMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/deep-belief-network/dbn/models.py"
      ],
      "metadata": {
        "id": "iDMlQN9sQxms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import atexit\n",
        "from abc import ABCMeta\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import numpy as np\n",
        "#import tensorflow as tf\n",
        "from sklearn.base import ClassifierMixin, RegressorMixin\n",
        "\n",
        "#from ..models import AbstractSupervisedDBN as BaseAbstractSupervisedDBN\n",
        "#from ..models import BaseModel\n",
        "#from ..models import BinaryRBM as BaseBinaryRBM\n",
        "#from ..models import UnsupervisedDBN as BaseUnsupervisedDBN\n",
        "#from ..utils import batch_generator, to_categorical\n",
        "\n",
        "\n",
        "def close_session():\n",
        "    sess.close()\n",
        "\n",
        "\n",
        "sess = tf.Session()\n",
        "atexit.register(close_session)\n",
        "\n",
        "\n",
        "def weight_variable(func, shape, stddev, dtype=tf.float32):\n",
        "    initial = func(shape, stddev=stddev, dtype=dtype)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "\n",
        "def bias_variable(value, shape, dtype=tf.float32):\n",
        "    initial = tf.constant(value, shape=shape, dtype=dtype)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "\n",
        "class BaseTensorFlowModel(BaseModel):\n",
        "    def save(self, save_path):\n",
        "        import pickle\n",
        "\n",
        "        with open(save_path, 'wb') as fp:\n",
        "            pickle.dump(self.to_dict(), fp)\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, load_path):\n",
        "        import pickle\n",
        "\n",
        "        with open(load_path, 'rb') as fp:\n",
        "            dct_to_load = pickle.load(fp)\n",
        "            return cls.from_dict(dct_to_load)\n",
        "\n",
        "    def to_dict(self):\n",
        "        dct_to_save = {name: self.__getattribute__(name) for name in self._get_param_names()}\n",
        "        dct_to_save.update(\n",
        "            {name: self.__getattribute__(name).eval(sess) for name in self._get_weight_variables_names()})\n",
        "        return dct_to_save\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls, dct_to_load):\n",
        "        pass\n",
        "\n",
        "    def _build_model(self, weights=None):\n",
        "        pass\n",
        "\n",
        "    def _initialize_weights(self, weights):\n",
        "        pass\n",
        "\n",
        "    @classmethod\n",
        "    def _get_weight_variables_names(cls):\n",
        "        pass\n",
        "\n",
        "    @classmethod\n",
        "    def _get_param_names(cls):\n",
        "        pass\n",
        "\n",
        "\n",
        "class BinaryRBM(BaseBinaryRBM, BaseTensorFlowModel):\n",
        "    \"\"\"\n",
        "    This class implements a Binary Restricted Boltzmann machine based on TensorFlow.\n",
        "    \"\"\"\n",
        "\n",
        "    def fit(self, X):\n",
        "        \"\"\"\n",
        "        Fit a model given data.\n",
        "        :param X: array-like, shape = (n_samples, n_features)\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        self.n_visible_units = X.shape[1]\n",
        "\n",
        "        # Initialize RBM parameters\n",
        "        self._build_model()\n",
        "\n",
        "        sess.run(tf.variables_initializer([self.W, self.c, self.b]))\n",
        "\n",
        "        if self.optimization_algorithm == 'sgd':\n",
        "            self._stochastic_gradient_descent(X)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid optimization algorithm.\")\n",
        "        return\n",
        "\n",
        "    @classmethod\n",
        "    def _get_weight_variables_names(cls):\n",
        "        return ['W', 'c', 'b']\n",
        "\n",
        "    @classmethod\n",
        "    def _get_param_names(cls):\n",
        "        return ['n_hidden_units',\n",
        "                'n_visible_units',\n",
        "                'activation_function',\n",
        "                'optimization_algorithm',\n",
        "                'learning_rate',\n",
        "                'n_epochs',\n",
        "                'contrastive_divergence_iter',\n",
        "                'batch_size',\n",
        "                'verbose',\n",
        "                '_activation_function_class']\n",
        "\n",
        "    def _initialize_weights(self, weights):\n",
        "        if weights:\n",
        "            for attr_name, value in weights.items():\n",
        "                self.__setattr__(attr_name, tf.Variable(value))\n",
        "        else:\n",
        "            if self.activation_function == 'sigmoid':\n",
        "                stddev = 1.0 / np.sqrt(self.n_visible_units)\n",
        "                self.W = weight_variable(tf.random_normal, [self.n_hidden_units, self.n_visible_units], stddev)\n",
        "                self.c = weight_variable(tf.random_normal, [self.n_hidden_units], stddev)\n",
        "                self.b = weight_variable(tf.random_normal, [self.n_visible_units], stddev)\n",
        "                self._activation_function_class = tf.nn.sigmoid\n",
        "            elif self.activation_function == 'relu':\n",
        "                stddev = 0.1 / np.sqrt(self.n_visible_units)\n",
        "                self.W = weight_variable(tf.truncated_normal, [self.n_hidden_units, self.n_visible_units], stddev)\n",
        "                self.c = bias_variable(stddev, [self.n_hidden_units])\n",
        "                self.b = bias_variable(stddev, [self.n_visible_units])\n",
        "                self._activation_function_class = tf.nn.relu\n",
        "            else:\n",
        "                raise ValueError(\"Invalid activation function.\")\n",
        "\n",
        "    def _build_model(self, weights=None):\n",
        "        \"\"\"\n",
        "        Builds TensorFlow model.\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        # initialize weights and biases\n",
        "        self._initialize_weights(weights)\n",
        "\n",
        "        # TensorFlow operations\n",
        "        self.visible_units_placeholder = tf.placeholder(tf.float32, shape=[None, self.n_visible_units])\n",
        "        self.compute_hidden_units_op = self._activation_function_class(\n",
        "            tf.transpose(tf.matmul(self.W, tf.transpose(self.visible_units_placeholder))) + self.c)\n",
        "        self.hidden_units_placeholder = tf.placeholder(tf.float32, shape=[None, self.n_hidden_units])\n",
        "        self.compute_visible_units_op = self._activation_function_class(\n",
        "            tf.matmul(self.hidden_units_placeholder, self.W) + self.b)\n",
        "        self.random_uniform_values = tf.Variable(tf.random_uniform([self.batch_size, self.n_hidden_units]))\n",
        "        sample_hidden_units_op = tf.to_float(self.random_uniform_values < self.compute_hidden_units_op)\n",
        "        self.random_variables = [self.random_uniform_values]\n",
        "\n",
        "        # Positive gradient\n",
        "        # Outer product. N is the batch size length.\n",
        "        # From http://stackoverflow.com/questions/35213787/tensorflow-batch-outer-product\n",
        "        positive_gradient_op = tf.matmul(tf.expand_dims(sample_hidden_units_op, 2),  # [N, U, 1]\n",
        "                                         tf.expand_dims(self.visible_units_placeholder, 1))  # [N, 1, V]\n",
        "\n",
        "        # Negative gradient\n",
        "        # Gibbs sampling\n",
        "        sample_hidden_units_gibbs_step_op = sample_hidden_units_op\n",
        "        for t in range(self.contrastive_divergence_iter):\n",
        "            compute_visible_units_op = self._activation_function_class(\n",
        "                tf.matmul(sample_hidden_units_gibbs_step_op, self.W) + self.b)\n",
        "            compute_hidden_units_gibbs_step_op = self._activation_function_class(\n",
        "                tf.transpose(tf.matmul(self.W, tf.transpose(compute_visible_units_op))) + self.c)\n",
        "            random_uniform_values = tf.Variable(tf.random_uniform([self.batch_size, self.n_hidden_units]))\n",
        "            sample_hidden_units_gibbs_step_op = tf.to_float(random_uniform_values < compute_hidden_units_gibbs_step_op)\n",
        "            self.random_variables.append(random_uniform_values)\n",
        "\n",
        "        negative_gradient_op = tf.matmul(tf.expand_dims(sample_hidden_units_gibbs_step_op, 2),  # [N, U, 1]\n",
        "                                         tf.expand_dims(compute_visible_units_op, 1))  # [N, 1, V]\n",
        "\n",
        "        compute_delta_W = tf.reduce_mean(positive_gradient_op - negative_gradient_op, 0)\n",
        "        compute_delta_b = tf.reduce_mean(self.visible_units_placeholder - compute_visible_units_op, 0)\n",
        "        compute_delta_c = tf.reduce_mean(sample_hidden_units_op - sample_hidden_units_gibbs_step_op, 0)\n",
        "\n",
        "        self.update_W = tf.assign_add(self.W, self.learning_rate * compute_delta_W)\n",
        "        self.update_b = tf.assign_add(self.b, self.learning_rate * compute_delta_b)\n",
        "        self.update_c = tf.assign_add(self.c, self.learning_rate * compute_delta_c)\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls, dct_to_load):\n",
        "        weights = {var_name: dct_to_load.pop(var_name) for var_name in cls._get_weight_variables_names()}\n",
        "\n",
        "        _activation_function_class = dct_to_load.pop('_activation_function_class')\n",
        "        n_visible_units = dct_to_load.pop('n_visible_units')\n",
        "\n",
        "        instance = cls(**dct_to_load)\n",
        "        setattr(instance, '_activation_function_class', _activation_function_class)\n",
        "        setattr(instance, 'n_visible_units', n_visible_units)\n",
        "\n",
        "        # Initialize RBM parameters\n",
        "        instance._build_model(weights)\n",
        "        sess.run(tf.variables_initializer([getattr(instance, name) for name in cls._get_weight_variables_names()]))\n",
        "\n",
        "        return instance\n",
        "\n",
        "    def _stochastic_gradient_descent(self, _data):\n",
        "        \"\"\"\n",
        "        Performs stochastic gradient descend optimization algorithm.\n",
        "        :param _data: array-like, shape = (n_samples, n_features)\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        for iteration in range(1, self.n_epochs + 1):\n",
        "            idx = np.random.permutation(len(_data))\n",
        "            data = _data[idx]\n",
        "            for batch in batch_generator(self.batch_size, data):\n",
        "                if len(batch) < self.batch_size:\n",
        "                    # Pad with zeros\n",
        "                    pad = np.zeros((self.batch_size - batch.shape[0], batch.shape[1]), dtype=batch.dtype)\n",
        "                    batch = np.vstack((batch, pad))\n",
        "                sess.run(tf.variables_initializer(self.random_variables))  # Need to re-sample from uniform distribution\n",
        "                sess.run([self.update_W, self.update_b, self.update_c],\n",
        "                         feed_dict={self.visible_units_placeholder: batch})\n",
        "            if self.verbose:\n",
        "                error = self._compute_reconstruction_error(data)\n",
        "                print(\">> Epoch %d finished \\tRBM Reconstruction error %f\" % (iteration, error))\n",
        "\n",
        "    def _compute_hidden_units_matrix(self, matrix_visible_units):\n",
        "        \"\"\"\n",
        "        Computes hidden unit outputs.\n",
        "        :param matrix_visible_units: array-like, shape = (n_samples, n_features)\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        return sess.run(self.compute_hidden_units_op,\n",
        "                        feed_dict={self.visible_units_placeholder: matrix_visible_units})\n",
        "\n",
        "    def _compute_visible_units_matrix(self, matrix_hidden_units):\n",
        "        \"\"\"\n",
        "        Computes visible (or input) unit outputs.\n",
        "        :param matrix_hidden_units: array-like, shape = (n_samples, n_features)\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        return sess.run(self.compute_visible_units_op,\n",
        "                        feed_dict={self.hidden_units_placeholder: matrix_hidden_units})\n",
        "\n",
        "\n",
        "class UnsupervisedDBN(BaseUnsupervisedDBN, BaseTensorFlowModel):\n",
        "    \"\"\"\n",
        "    This class implements a unsupervised Deep Belief Network in TensorFlow\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(UnsupervisedDBN, self).__init__(**kwargs)\n",
        "        self.rbm_class = BinaryRBM\n",
        "\n",
        "    @classmethod\n",
        "    def _get_param_names(cls):\n",
        "        return ['hidden_layers_structure',\n",
        "                'activation_function',\n",
        "                'optimization_algorithm',\n",
        "                'learning_rate_rbm',\n",
        "                'n_epochs_rbm',\n",
        "                'contrastive_divergence_iter',\n",
        "                'batch_size',\n",
        "                'verbose']\n",
        "\n",
        "    @classmethod\n",
        "    def _get_weight_variables_names(cls):\n",
        "        return []\n",
        "\n",
        "    def to_dict(self):\n",
        "        dct_to_save = super(UnsupervisedDBN, self).to_dict()\n",
        "        dct_to_save['rbm_layers'] = [rbm.to_dict() for rbm in self.rbm_layers]\n",
        "        return dct_to_save\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls, dct_to_load):\n",
        "        rbm_layers = dct_to_load.pop('rbm_layers')\n",
        "        instance = cls(**dct_to_load)\n",
        "        setattr(instance, 'rbm_layers', [instance.rbm_class.from_dict(rbm) for rbm in rbm_layers])\n",
        "        return instance\n",
        "\n",
        "\n",
        "class TensorFlowAbstractSupervisedDBN(BaseAbstractSupervisedDBN, BaseTensorFlowModel):\n",
        "    __metaclass__ = ABCMeta\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(TensorFlowAbstractSupervisedDBN, self).__init__(UnsupervisedDBN, **kwargs)\n",
        "\n",
        "    @classmethod\n",
        "    def _get_param_names(cls):\n",
        "        return ['n_iter_backprop',\n",
        "                'l2_regularization',\n",
        "                'learning_rate',\n",
        "                'batch_size',\n",
        "                'dropout_p',\n",
        "                'verbose']\n",
        "\n",
        "    @classmethod\n",
        "    def _get_weight_variables_names(cls):\n",
        "        return ['W', 'b']\n",
        "\n",
        "    def _initialize_weights(self, weights):\n",
        "        if weights:\n",
        "            for attr_name, value in weights.items():\n",
        "                self.__setattr__(attr_name, tf.Variable(value))\n",
        "        else:\n",
        "            if self.unsupervised_dbn.activation_function == 'sigmoid':\n",
        "                stddev = 1.0 / np.sqrt(self.input_units)\n",
        "                self.W = weight_variable(tf.random_normal, [self.input_units, self.num_classes], stddev)\n",
        "                self.b = weight_variable(tf.random_normal, [self.num_classes], stddev)\n",
        "                self._activation_function_class = tf.nn.sigmoid\n",
        "            elif self.unsupervised_dbn.activation_function == 'relu':\n",
        "                stddev = 0.1 / np.sqrt(self.input_units)\n",
        "                self.W = weight_variable(tf.truncated_normal, [self.input_units, self.num_classes], stddev)\n",
        "                self.b = bias_variable(stddev, [self.num_classes])\n",
        "                self._activation_function_class = tf.nn.relu\n",
        "            else:\n",
        "                raise ValueError(\"Invalid activation function.\")\n",
        "\n",
        "    def to_dict(self):\n",
        "        dct_to_save = super(TensorFlowAbstractSupervisedDBN, self).to_dict()\n",
        "        dct_to_save['unsupervised_dbn'] = self.unsupervised_dbn.to_dict()\n",
        "        dct_to_save['num_classes'] = self.num_classes\n",
        "        return dct_to_save\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls, dct_to_load):\n",
        "        weights = {var_name: dct_to_load.pop(var_name) for var_name in cls._get_weight_variables_names()}\n",
        "        unsupervised_dbn_dct = dct_to_load.pop('unsupervised_dbn')\n",
        "        num_classes = dct_to_load.pop('num_classes')\n",
        "\n",
        "        instance = cls(**dct_to_load)\n",
        "\n",
        "        setattr(instance, 'unsupervised_dbn', instance.unsupervised_dbn_class.from_dict(unsupervised_dbn_dct))\n",
        "        setattr(instance, 'num_classes', num_classes)\n",
        "\n",
        "        # Initialize RBM parameters\n",
        "        instance._build_model(weights)\n",
        "        sess.run(tf.variables_initializer([getattr(instance, name) for name in cls._get_weight_variables_names()]))\n",
        "        return instance\n",
        "\n",
        "    def _build_model(self, weights=None):\n",
        "        self.visible_units_placeholder = self.unsupervised_dbn.rbm_layers[0].visible_units_placeholder\n",
        "        keep_prob = tf.placeholder(tf.float32)\n",
        "        visible_units_placeholder_drop = tf.nn.dropout(self.visible_units_placeholder, keep_prob)\n",
        "        self.keep_prob_placeholders = [keep_prob]\n",
        "\n",
        "        # Define tensorflow operation for a forward pass\n",
        "        rbm_activation = visible_units_placeholder_drop\n",
        "        for rbm in self.unsupervised_dbn.rbm_layers:\n",
        "            rbm_activation = rbm._activation_function_class(\n",
        "                tf.transpose(tf.matmul(rbm.W, tf.transpose(rbm_activation))) + rbm.c)\n",
        "            keep_prob = tf.placeholder(tf.float32)\n",
        "            self.keep_prob_placeholders.append(keep_prob)\n",
        "            rbm_activation = tf.nn.dropout(rbm_activation, keep_prob)\n",
        "\n",
        "        self.transform_op = rbm_activation\n",
        "        self.input_units = self.unsupervised_dbn.rbm_layers[-1].n_hidden_units\n",
        "\n",
        "        # weights and biases\n",
        "        self._initialize_weights(weights)\n",
        "\n",
        "        if self.unsupervised_dbn.optimization_algorithm == 'sgd':\n",
        "            self.optimizer = tf.train.GradientDescentOptimizer(self.learning_rate)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid optimization algorithm.\")\n",
        "\n",
        "        # operations\n",
        "        self.y = tf.matmul(self.transform_op, self.W) + self.b\n",
        "        self.y_ = tf.placeholder(tf.float32, shape=[None, self.num_classes])\n",
        "        self.train_step = None\n",
        "        self.cost_function = None\n",
        "        self.output = None\n",
        "\n",
        "    def _fine_tuning(self, data, _labels):\n",
        "        self.num_classes = self._determine_num_output_neurons(_labels)\n",
        "        if self.num_classes == 1:\n",
        "            _labels = np.expand_dims(_labels, -1)\n",
        "\n",
        "        self._build_model()\n",
        "        sess.run(tf.variables_initializer([self.W, self.b]))\n",
        "\n",
        "        labels = self._transform_labels_to_network_format(_labels)\n",
        "\n",
        "        if self.verbose:\n",
        "            print(\"[START] Fine tuning step:\")\n",
        "        self._stochastic_gradient_descent(data, labels)\n",
        "        if self.verbose:\n",
        "            print(\"[END] Fine tuning step\")\n",
        "\n",
        "    def _stochastic_gradient_descent(self, data, labels):\n",
        "        for iteration in range(self.n_iter_backprop):\n",
        "            for batch_data, batch_labels in batch_generator(self.batch_size, data, labels):\n",
        "                feed_dict = {self.visible_units_placeholder: batch_data,\n",
        "                             self.y_: batch_labels}\n",
        "                feed_dict.update({placeholder: self.p for placeholder in self.keep_prob_placeholders})\n",
        "                sess.run(self.train_step, feed_dict=feed_dict)\n",
        "\n",
        "            if self.verbose:\n",
        "                feed_dict = {self.visible_units_placeholder: data, self.y_: labels}\n",
        "                feed_dict.update({placeholder: 1.0 for placeholder in self.keep_prob_placeholders})\n",
        "                error = sess.run(self.cost_function, feed_dict=feed_dict)\n",
        "                print(\">> Epoch %d finished \\tANN training loss %f\" % (iteration, error))\n",
        "\n",
        "    def transform(self, X):\n",
        "        feed_dict = {self.visible_units_placeholder: X}\n",
        "        feed_dict.update({placeholder: 1.0 for placeholder in self.keep_prob_placeholders})\n",
        "        return sess.run(self.transform_op,\n",
        "                        feed_dict=feed_dict)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predicts the target given data.\n",
        "        :param X: array-like, shape = (n_samples, n_features)\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        if len(X.shape) == 1:  # It is a single sample\n",
        "            X = np.expand_dims(X, 0)\n",
        "        predicted_data = self._compute_output_units_matrix(X)\n",
        "        return predicted_data\n",
        "\n",
        "    def _compute_output_units_matrix(self, matrix_visible_units):\n",
        "        feed_dict = {self.visible_units_placeholder: matrix_visible_units}\n",
        "        feed_dict.update({placeholder: 1.0 for placeholder in self.keep_prob_placeholders})\n",
        "        return sess.run(self.output, feed_dict=feed_dict)\n",
        "\n",
        "\n",
        "class SupervisedDBNClassification(TensorFlowAbstractSupervisedDBN, ClassifierMixin):\n",
        "    \"\"\"\n",
        "    This class implements a Deep Belief Network for classification problems.\n",
        "    It appends a Softmax Linear Classifier as output layer.\n",
        "    \"\"\"\n",
        "\n",
        "    def _build_model(self, weights=None):\n",
        "        super(SupervisedDBNClassification, self)._build_model(weights)\n",
        "        self.output = tf.nn.softmax(self.y)\n",
        "        self.cost_function = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.y, labels=tf.stop_gradient(self.y_)))\n",
        "        self.train_step = self.optimizer.minimize(self.cost_function)\n",
        "\n",
        "    @classmethod\n",
        "    def _get_param_names(cls):\n",
        "        return super(SupervisedDBNClassification, cls)._get_param_names() + ['label_to_idx_map', 'idx_to_label_map']\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls, dct_to_load):\n",
        "        label_to_idx_map = dct_to_load.pop('label_to_idx_map')\n",
        "        idx_to_label_map = dct_to_load.pop('idx_to_label_map')\n",
        "\n",
        "        instance = super(SupervisedDBNClassification, cls).from_dict(dct_to_load)\n",
        "        setattr(instance, 'label_to_idx_map', label_to_idx_map)\n",
        "        setattr(instance, 'idx_to_label_map', idx_to_label_map)\n",
        "\n",
        "        return instance\n",
        "\n",
        "    def _transform_labels_to_network_format(self, labels):\n",
        "        new_labels, label_to_idx_map, idx_to_label_map = to_categorical(labels, self.num_classes)\n",
        "        self.label_to_idx_map = label_to_idx_map\n",
        "        self.idx_to_label_map = idx_to_label_map\n",
        "        return new_labels\n",
        "\n",
        "    def _transform_network_format_to_labels(self, indexes):\n",
        "        \"\"\"\n",
        "        Converts network output to original labels.\n",
        "        :param indexes: array-like, shape = (n_samples, )\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        return list(map(lambda idx: self.idx_to_label_map[idx], indexes))\n",
        "\n",
        "    def predict(self, X):\n",
        "        probs = self.predict_proba(X)\n",
        "        indexes = np.argmax(probs, axis=1)\n",
        "        return self._transform_network_format_to_labels(indexes)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Predicts probability distribution of classes for each sample in the given data.\n",
        "        :param X: array-like, shape = (n_samples, n_features)\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        return super(SupervisedDBNClassification, self)._compute_output_units_matrix(X)\n",
        "\n",
        "    def predict_proba_dict(self, X):\n",
        "        \"\"\"\n",
        "        Predicts probability distribution of classes for each sample in the given data.\n",
        "        Returns a list of dictionaries, one per sample. Each dict contains {label_1: prob_1, ..., label_j: prob_j}\n",
        "        :param X: array-like, shape = (n_samples, n_features)\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        if len(X.shape) == 1:  # It is a single sample\n",
        "            X = np.expand_dims(X, 0)\n",
        "\n",
        "        predicted_probs = self.predict_proba(X)\n",
        "\n",
        "        result = []\n",
        "        num_of_data, num_of_labels = predicted_probs.shape\n",
        "        for i in range(num_of_data):\n",
        "            # key : label\n",
        "            # value : predicted probability\n",
        "            dict_prob = {}\n",
        "            for j in range(num_of_labels):\n",
        "                dict_prob[self.idx_to_label_map[j]] = predicted_probs[i][j]\n",
        "            result.append(dict_prob)\n",
        "\n",
        "        return result\n",
        "\n",
        "    def _determine_num_output_neurons(self, labels):\n",
        "        return len(np.unique(labels))\n",
        "\n",
        "\n",
        "class SupervisedDBNRegression(TensorFlowAbstractSupervisedDBN, RegressorMixin):\n",
        "    \"\"\"\n",
        "    This class implements a Deep Belief Network for regression problems in TensorFlow.\n",
        "    \"\"\"\n",
        "\n",
        "    def _build_model(self, weights=None):\n",
        "        super(SupervisedDBNRegression, self)._build_model(weights)\n",
        "        self.output = self.y\n",
        "        self.cost_function = tf.reduce_mean(tf.square(self.y_ - self.y))  # Mean Squared Error\n",
        "        self.train_step = self.optimizer.minimize(self.cost_function)\n",
        "\n",
        "    def _transform_labels_to_network_format(self, labels):\n",
        "        \"\"\"\n",
        "        Returns the same labels since regression case does not need to convert anything.\n",
        "        :param labels: array-like, shape = (n_samples, targets)\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        return labels\n",
        "\n",
        "    def _compute_output_units_matrix(self, matrix_visible_units):\n",
        "        return super(SupervisedDBNRegression, self)._compute_output_units_matrix(matrix_visible_units)\n",
        "\n",
        "    def _determine_num_output_neurons(self, labels):\n",
        "        if len(labels.shape) == 1:\n",
        "            return 1\n",
        "        else:\n",
        "            return labels.shape[1]\n"
      ],
      "metadata": {
        "id": "1oTCiF6NRMeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TabNet 3.0 "
      ],
      "metadata": {
        "id": "Rk5-twnyedJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pytorch-tabnet"
      ],
      "metadata": {
        "id": "VRphAhKLeimn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\n",
        "\n",
        "clf = TabNetClassifier()  #TabNetRegressor()\n",
        "clf.fit(\n",
        "  train_X, train_Y,\n",
        "  eval_set=[(test_X, test_Y)]\n",
        ")\n",
        "#preds = clf.predict(X_test)"
      ],
      "metadata": {
        "id": "1vifg3nIeu2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_tabnet.multitask import TabNetMultiTaskClassifier\n",
        "clf = TabNetMultiTaskClassifier()\n",
        "clf.fit(\n",
        "  train_X, train_Y,\n",
        "  eval_set=[(test_X, test_Y)]\n",
        ")\n",
        "#preds = clf.predict(X_test)"
      ],
      "metadata": {
        "id": "-2n-xbZsh8k9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DBN 2.0"
      ],
      "metadata": {
        "id": "Hr9KvKmfkW_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from dbn.tensorflow import SupervisedDBNClassification\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics.classification import accuracy_score"
      ],
      "metadata": {
        "id": "vnlZ5ZuKkZi0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93f2671f",
      "metadata": {
        "id": "93f2671f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d28293d",
      "metadata": {
        "id": "2d28293d"
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv(\"dataset_phishing.csv\",index_col=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a21147c",
      "metadata": {
        "id": "0a21147c"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a368098b",
      "metadata": {
        "id": "a368098b"
      },
      "outputs": [],
      "source": [
        "df['target'] = pd.get_dummies(df['status'])['legitimate'].astype('int')\n",
        "df.drop('status',axis = 1, inplace=True)\n",
        "df[['url','target']].head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97e2931c",
      "metadata": {
        "id": "97e2931c"
      },
      "outputs": [],
      "source": [
        "tmp = df.isnull().sum().reset_index(name='missing_val')\n",
        "tmp[tmp['missing_val']!= 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e3a745c",
      "metadata": {
        "id": "4e3a745c"
      },
      "outputs": [],
      "source": [
        "#Train & Test Set\n",
        "X= df.iloc[: , 1:-1]\n",
        "#y = upsampled_df['Churn']\n",
        "y= df['target']\n",
        "\n",
        "train_x,test_x,train_y,test_y = train_test_split(X,y,random_state=42)\n",
        "print(\"\\n--Training data samples--\")\n",
        "print(train_x.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9a3c465",
      "metadata": {
        "id": "a9a3c465"
      },
      "outputs": [],
      "source": [
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b89275dd",
      "metadata": {
        "id": "b89275dd"
      },
      "outputs": [],
      "source": [
        "###First use a MinMaxscaler to scale all the features of Train & Test dataframes\n",
        "\n",
        "scaler = preprocessing.MinMaxScaler()\n",
        "x_train = scaler.fit_transform(train_x.values)\n",
        "x_test =  scaler.fit_transform(test_x.values)\n",
        "\n",
        "print(\"Scaled values of Train set \\n\")\n",
        "print(x_train)\n",
        "print(\"\\nScaled values of Test set \\n\")\n",
        "print(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.columns"
      ],
      "metadata": {
        "id": "sreskmbqWHlu"
      },
      "id": "sreskmbqWHlu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "053c743f",
      "metadata": {
        "id": "053c743f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4f01734",
      "metadata": {
        "id": "c4f01734"
      },
      "outputs": [],
      "source": [
        "###Then convert the Train and Test sets into Tensors\n",
        "\n",
        "x_tensor =  torch.from_numpy(x_train).float()\n",
        "y_tensor =  torch.from_numpy(train_y.values.ravel()).float()\n",
        "xtest_tensor =  torch.from_numpy(x_test).float()\n",
        "ytest_tensor =  torch.from_numpy(test_y.values.ravel()).float()\n",
        "\n",
        "print(\"\\nTrain set Tensors \\n\")\n",
        "print(x_tensor)\n",
        "print(y_tensor)\n",
        "print(\"\\nTest set Tensors \\n\")\n",
        "print(xtest_tensor)\n",
        "print(ytest_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bed346f1",
      "metadata": {
        "id": "bed346f1"
      },
      "outputs": [],
      "source": [
        "#Define a batch size , \n",
        "bs = 64\n",
        "#Both x_train and y_train can be combined in a single TensorDataset, which will be easier to iterate over and slice\n",
        "y_tensor = y_tensor.unsqueeze(1)\n",
        "train_ds = TensorDataset(x_tensor, y_tensor)\n",
        "#Pytorchâ€™s DataLoader is responsible for managing batches. \n",
        "#You can create a DataLoader from any Dataset. DataLoader makes it easier to iterate over batches\n",
        "train_dl = DataLoader(train_ds, batch_size=bs)\n",
        "\n",
        "\n",
        "#For the validation/test dataset\n",
        "ytest_tensor = ytest_tensor.unsqueeze(1)\n",
        "test_ds = TensorDataset(xtest_tensor, ytest_tensor)\n",
        "test_loader = DataLoader(test_ds, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9b77b6f",
      "metadata": {
        "id": "c9b77b6f"
      },
      "outputs": [],
      "source": [
        "n_input_dim = train_x.shape[1]\n",
        "\n",
        "#Layer size\n",
        "n_hidden1 = 300  # Number of hidden nodes\n",
        "n_hidden2 = 100\n",
        "n_output =  1   # Number of output nodes = for binary classifier\n",
        "\n",
        "\n",
        "class ChurnModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ChurnModel, self).__init__()\n",
        "        self.layer_1 = nn.Linear(n_input_dim, n_hidden1) \n",
        "        self.layer_2 = nn.Linear(n_hidden1, n_hidden2)\n",
        "        self.layer_out = nn.Linear(n_hidden2, n_output) \n",
        "        \n",
        "        \n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid =  nn.Sigmoid()\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "        self.batchnorm1 = nn.BatchNorm1d(n_hidden1)\n",
        "        self.batchnorm2 = nn.BatchNorm1d(n_hidden2)\n",
        "        \n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        x = self.relu(self.layer_1(inputs))\n",
        "        x = self.batchnorm1(x)\n",
        "        x = self.relu(self.layer_2(x))\n",
        "        x = self.batchnorm2(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.sigmoid(self.layer_out(x))\n",
        "        \n",
        "        return x\n",
        "    \n",
        "\n",
        "model = ChurnModel()\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03f64640",
      "metadata": {
        "id": "03f64640"
      },
      "outputs": [],
      "source": [
        "#Loss Computation\n",
        "loss_func = nn.BCELoss()\n",
        "#Optimizer\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "epochs = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36e3a9ca",
      "metadata": {
        "id": "36e3a9ca"
      },
      "outputs": [],
      "source": [
        "model.train()\n",
        "train_loss = []\n",
        "for epoch in range(epochs):\n",
        "    #Within each epoch run the subsets of data = batch sizes.\n",
        "    for xb, yb in train_dl:\n",
        "        y_pred = model(xb)            # Forward Propagation\n",
        "        loss = loss_func(y_pred, yb)  # Loss Computation\n",
        "        optimizer.zero_grad()         # Clearing all previous gradients, setting to zero \n",
        "        loss.backward()               # Back Propagation\n",
        "        optimizer.step()              # Updating the parameters \n",
        "    #print(\"Loss in iteration :\"+str(epoch)+\" is: \"+str(loss.item()))\n",
        "    train_loss.append(loss.item())\n",
        "print('Last iteration loss value: '+str(loss.item()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86f81222",
      "metadata": {
        "id": "86f81222"
      },
      "outputs": [],
      "source": [
        "plt.plot(train_loss)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f84c016b",
      "metadata": {
        "id": "f84c016b"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "\n",
        "y_pred_list = []\n",
        "model.eval()\n",
        "#Since we don't need model to back propagate the gradients in test set we use torch.no_grad()\n",
        "# reduces memory usage and speeds up computation\n",
        "with torch.no_grad():\n",
        "    for xb_test,yb_test  in test_loader:\n",
        "        y_test_pred = model(xb_test)\n",
        "        y_pred_tag = torch.round(y_test_pred)\n",
        "        y_pred_list.append(y_pred_tag.detach().numpy())\n",
        "\n",
        "#Takes arrays and makes them list of list for each batch        \n",
        "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
        "#flattens the lists in sequence\n",
        "ytest_pred = list(itertools.chain.from_iterable(y_pred_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ba45328",
      "metadata": {
        "id": "1ba45328"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bdd6802",
      "metadata": {
        "id": "1bdd6802"
      },
      "outputs": [],
      "source": [
        "y_true_test = test_y.values.ravel()\n",
        "conf_matrix = confusion_matrix(y_true_test ,ytest_pred)\n",
        "print(\"Confusion Matrix of the Test Set\")\n",
        "print(\"-----------\")\n",
        "print(conf_matrix)\n",
        "print(\"Precision of the MLP :\\t\"+str(precision_score(y_true_test,ytest_pred)))\n",
        "print(\"Recall of the MLP    :\\t\"+str(recall_score(y_true_test,ytest_pred)))\n",
        "print(\"F1 Score of the Model :\\t\"+str(f1_score(y_true_test,ytest_pred)))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "MLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}